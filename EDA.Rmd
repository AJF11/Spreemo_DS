---
title: "Data Analysis of Exam Data"
author: "Alan Feder"
date: "November 1, 2016"
output:
  html_notebook: default
  html_document: default
---

***

## Data Science Exercise for Spreemo

***

### Introduction

The purpose of this analysis is to look at reviews of exams and use them to assess the quality of providers.  Furthermore, we look at attributes of the providers, such as imaging equipment and sub-specializations.

The following analysis is done in `R`, using the `RMarkdown` format.

### Setup

```{r setup, echo=FALSE, message=FALSE}
knitr::opts_chunk$set(
	echo = TRUE,
	message = TRUE,
	warning = TRUE
)
dataPath <- file.path(dirname(getwd()), 'data')
```

I like to start out by loading all necessary external packages.

```{r, message = FALSE}
library(dtplyr)
library(data.table)
library(splitstackshape)
library(ggplot2)
library(tidyr)
library(readr)
library(dplyr)
library(stringr)
library(scales)
library(boot)
```

I then read in all raw data files.

```{r read in data, message = FALSE}
exam_data <- read_csv(file.path(dataPath, 'ExamRatingData072516.csv'))
equipment_data <- read_csv(file.path(dataPath, 'ProviderEquipmentData072516.csv'))
subspecial_data <- read_csv(file.path(dataPath, 'ProviderSubspecializationData072516.csv'))
```

```{r, include=FALSE, eval = FALSE}
names(exam_data)
names(equipment_data)
names(subspecial_data)
```

### Exploratory Data Analysis

Let's start by taking a quick glance at the exam data file.

```{r}
head(exam_data, 10) %>% as.data.frame()
```

So we can see that the first column is an Exam ID, the second column is a Provider IS.  We can see that there are sometimes multiple Reviewer IDs for one exam.  We can also see that there are a number of missing variables.

```{r}
exam_data %>%
  is.na %>%
  colMeans
```

Certain variables -- such as the scores -- are complete, while other ones are missing more than 50% of the information. While some of the missing `RadPeer.Significance.of.Errors` are due to the reviewer not seeing any errors, it seems as if some exams have no information about correct/incorrect diagnoses.

I also wanted to see how many claims have multiple reviews.

```{r}
exam_data %>%
  count(Exam.ID, Provider.ID) %>%
  ungroup %>%
  count(n) %>%
  mutate(pct = percent(nn/ sum(nn))) %>%
  knitr::kable(align = 'c')
```

While most Exam IDs have only one reviewer, 16.5% have two, and none have more than two.

I want to look at the claims with multiple ratings.

```{r}
exam_data %>%
  count(Exam.ID, Provider.ID) %>%
  filter(n >1) %>%
  semi_join(exam_data, ., by = c("Exam.ID", "Provider.ID")) %>%
  select(c(1:3,13:16)) %>%
  group_by(Exam.ID, Provider.ID) %>%
  summarize_all(n_distinct) %>%
  ungroup %>%
  select(-(1:2)) %>%
  colSums
```

Luckily, each exam has two different reviewer IDs, and the Sex, Age, and body part are the same across all reviewers for the same exam. This could have been a source of data errors, but it seems to be complete.

#### Feature Creation

I also decided to create some accuracy metrics of my own.  I wanted to look at False Positive Rate, $FPR = \frac{FP}{FP+TN}$, False Negative Rate, $FNR = \frac{FN}{FN+TP}$, and total error rate $errR = \frac{FN+FP}{FN+FP+TN+TP}$.  I also created significance weighed versions of these metrics, where the error is 0 if `RadPeer.Significance.of.Errors` is 0 or `NA`.

```{r}
exam_data <- exam_data %>%
  mutate(Neg.Count = False.Positive.Count + True.Negative.Count,
         Pos.Count = False.Negative.Count + True.Positive.Count, 
         Tot.Count = Neg.Count + Pos.Count,
         FPR = False.Positive.Count / Neg.Count,
         FNR = False.Negative.Count / Pos.Count,
         errR = Total.Diagnostic.Errors / Tot.Count,
         sigErr = as.numeric(coalesce(RadPeer.Significance.of.Errors, 0L)),
         w.FPR = sigErr * FPR,
         w.FNR = sigErr * FNR,
         w.errR = sigErr * errR)
```


Therefore, to make sure each exam has equal weighting, I average the data for each exam that shows up multiple times. 

```{r}
each_exam_data1 <- exam_data %>%
  group_by(Exam.ID, Provider.ID) %>%
  summarise_at(c(4:5, 7:12, 17:26), mean, na.rm = T) %>%
  mutate_all(na_if, y= 'NaN')
each_exam_data1 <- exam_data %>%
  select(c(1:2, 13:16)) %>%
  distinct() %>%
  inner_join(each_exam_data1, by = c("Exam.ID", "Provider.ID"))
```

Now we can aggregate the exam date across each provider. I will use the mean to average them. 

```{r}
ProviderSummary <- each_exam_data1 %>% 
  group_by(Provider.ID) %>%
  summarize_at(vars(RadPeer.Score, Technical.Performance.Score), 
               mean, na.rm = T)
ProviderSummary <- each_exam_data1 %>%
  count(Provider.ID) %>%
  rename(nExams = n) %>%
  inner_join(ProviderSummary, by = "Provider.ID")
ProviderSummary <- each_exam_data1 %>% 
  group_by(Provider.ID) %>%
  summarize_at(vars(Total.Diagnostic.Errors, False.Positive.Count,
                    False.Negative.Count, True.Negative.Count, 
                    True.Positive.Count, Neg.Count, Pos.Count, Tot.Count), 
               sum, na.rm = T) %>%
  inner_join(ProviderSummary, ., by = 'Provider.ID')
ProviderSummary <- each_exam_data1 %>% 
  group_by(Provider.ID) %>%
  summarize(FPR = weighted.mean(FPR, Neg.Count, na.rm = T),
            w.FPR = weighted.mean(w.FPR, Neg.Count, na.rm = T),
            FNR = weighted.mean(FNR, Pos.Count, na.rm = T),
            w.FNR = weighted.mean(w.FNR, Pos.Count, na.rm = T),
            errR = weighted.mean(errR, Tot.Count, na.rm = T),
            w.errR = weighted.mean(w.errR, Tot.Count, na.rm = T)) %>%
  inner_join(ProviderSummary, ., by = 'Provider.ID')
```

####More Exploration

I want to start by looking at `RadPeer.Score` and `Technical.Performance.Score`, since the other measures have missing variables for many providers.  Before doing anything complicated, I am curious to look at the correlation between the two measures.

```{r}
ProviderSummary %>% 
  with(cor(x = RadPeer.Score, y = Technical.Performance.Score)) %>%
  round(3) %>%
  paste('Correlation of Means:', .)
```

```{r}
ProviderSummary %>% 
  ggplot(data = ., mapping = aes(x = RadPeer.Score, y = Technical.Performance.Score)) +
  geom_point() +
  geom_smooth(method = 'lm', se = FALSE) + 
  theme_light() +
  labs(title = 'Scatter plot of Provider Score')
```

These correlations are surprisingly low to me - while a positive correlation does make more sense than a negative one, I'd have expected it to be closer to 1 than it actually is.

One problem may be the many Providers that have very few exams, while others have many more.

```{r}
summary(ProviderSummary$nExams)
```

While 25% of Providers have only 1 exam, a different 25% have 6 or more exams, with one provider having as many as 56.  Now, I'd like to consider looking at the correlation statistic weighted by # of exams.

```{r}
ProviderSummary %>% 
  select(RadPeer.Score, Technical.Performance.Score) %>% 
  corr(w = ProviderSummary$nExams) %>% round(3) %>%
  paste('Weighted Correlation of Means:', .)
```

These correlations are a bit higher.  I'm still a bit surprised by how low they are, but it's not as tiny as before.

```{r}
ProviderSummary %>% 
  ggplot(data = ., mapping = aes(x = RadPeer.Score, y = Technical.Performance.Score)) +
  geom_point(aes(size = nExams)) +
  geom_smooth(method = 'lm', se = FALSE,
              aes(weight = ProviderSummary$nExams)) +
  theme_light() +
  labs(title = 'Scatter plot of Provider Score Means', size = '# Exams')
```

One interesting thing we can see from this scatter plot is that there are a lot of providers with scores clustered around 1.0 to 1.5 in both metrics, including some of those with the most exams.

The following is a look at weighted correlations between the official metrics (`RadPeer` and `Technical.Performance`) and the ones I created.

```{r}
corrWRP <- function(score1, col1, col2){
  df1 <- ProviderSummary %>%
    select_(score1, col1, col2) %>%
    filter_(paste0('!is.na(', col1, ')'))
  df1 %>% select_(score1, col1) %>%
    corr(w = df1[[col2]]) %>%
    round(3) %>%
    paste0('Weighted Correlation between ', score1,' and ', col1, ': ', .)
}
```

```{r}
corrWRP('RadPeer.Score', 'FPR', 'Neg.Count')
corrWRP('RadPeer.Score', 'w.FPR', 'Neg.Count')
corrWRP('RadPeer.Score', 'FNR', 'Pos.Count')
corrWRP('RadPeer.Score', 'w.FNR', 'Pos.Count')
corrWRP('RadPeer.Score', 'errR', 'Tot.Count')
corrWRP('RadPeer.Score', 'w.errR', 'Tot.Count')
```

```{r}
corrWRP('Technical.Performance.Score', 'FPR', 'Neg.Count')
corrWRP('Technical.Performance.Score', 'w.FPR', 'Neg.Count')
corrWRP('Technical.Performance.Score', 'FNR', 'Pos.Count')
corrWRP('Technical.Performance.Score', 'w.FNR', 'Pos.Count')
corrWRP('Technical.Performance.Score', 'errR', 'Tot.Count')
corrWRP('Technical.Performance.Score', 'w.errR', 'Tot.Count')
```

There are two things interesting to me about these correlations:

* Correlations seem better with the metrics that _do not_ correct for significance of errors than for those that do.
    + This is surprising since I would expect that insignificant errors wouldn't matter as much.
* The correlations are stronger with the `RadPeer` metric than the `Technical.Performance` metric. 
    + This does not surprise me as much, since the `RadPeer` metric is explicitly based on accuracy.  
    + However, I am surprised by how low the correlation with `Technical.Performance` is.

### Clustering
    
Now let's try to do a little clustering.  First, we will impute the missing values of `FPR`, `FNR`, etc... to the mean of each column, and then center and scale it.
    
```{r}
toScale <- ProviderSummary %>%
  summarize_at(vars(RadPeer.Score, Technical.Performance.Score, 
                    FPR, w.FPR, FNR, w.FNR, errR, w.errR), funs(mean, sd), na.rm = T)

data1 <- ProviderSummary %>%
  select(Provider.ID, nExams, RadPeer.Score, Technical.Performance.Score,
         FPR, w.FPR, FNR, w.FNR, errR, w.errR) %>%
  mutate(RadPeer.Score2 = (RadPeer.Score - toScale$RadPeer.Score_mean) / toScale$RadPeer.Score_sd,
         Technical.Performance.Score2 = (Technical.Performance.Score - toScale$Technical.Performance.Score_mean) / toScale$Technical.Performance.Score_sd,
         FPR2 = coalesce((FPR - toScale$FPR_mean) / toScale$FPR_sd, 0),
         FNR2 = coalesce((FNR - toScale$FNR_mean) / toScale$FNR_sd, 0),
         errR2 = coalesce((errR - toScale$errR_mean) / toScale$errR_sd, 0),
         w.FPR2 = coalesce((w.FPR - toScale$w.FPR_mean) / toScale$w.FPR_sd, 0),
         w.FNR2 = coalesce((w.FNR - toScale$w.FNR_mean) / toScale$w.FNR_sd, 0),
         w.errR2 = coalesce((w.errR - toScale$w.errR_mean) / toScale$errR_sd, 0))
```

```{r}
set.seed(8927489)
kMeans2 <- data1 %>% select(11:18) %>% kmeans(2, nstart = 20)
kMeans2
```

When creating this cluster, we can see that the clustering accounts for 28.4% of the sum of squares.  We can also see that cluster 1 is the "good" cluster, since the cluster means are lower.

```{r}
data1[['cluster1']] <- kMeans2$cluster
data1 <- data1 %>% mutate(cluster1 = if_else(cluster1 == 1, 'good', 'bad'))
```

We can also create a different cluster by weighting each point by the number of exams.  We do this by expanding each Provider to the size of its number of exams.

```{r}
data2 <- data1 %>% select(c(1:2, 11:18)) %>% expandRows('nExams')
kMeans2a <- data2 %>%
  select(-Provider.ID) %>%
  kmeans(2, nstart = 20)
kMeans2a
```

Here, we account for 28.8% of the sum of squares, which is negligably higher.  It is also more balanced, as `r scales::percent(kMeans2a$size[[2]]/sum(kMeans2a$size))` of the providers are in the smaller bucket, as opposed to `r scales::percent(kMeans2$size[[2]]/sum(kMeans2$size))` in the larger bucket.  Bucket 1 is the "good" cluster again.

```{r}
data2[['cluster2']] <- kMeans2a$cluster
clust2 <- data2 %>% distinct(Provider.ID, cluster2) %>%
  mutate(cluster2 = if_else(cluster2 == 1, 'good', 'bad'))
data1 <- data1 %>% left_join(clust2, by = "Provider.ID")
```


```{r}
centers1 <- kMeans2a$centers[,1:2] %>% 
  data.frame %>%
  mutate(RadPeer.Score = (RadPeer.Score2 * toScale$RadPeer.Score_sd) + toScale$RadPeer.Score_mean, 
         Technical.Performance.Score = (Technical.Performance.Score2 * toScale$Technical.Performance.Score_sd) + toScale$Technical.Performance.Score_mean,
         cluster2 = c('good', 'bad'))
ggplot(data1, aes(x = RadPeer.Score, y = Technical.Performance.Score, color = cluster2)) +
  geom_point(aes(size = nExams)) +
  theme_light() +
  geom_point(data = centers1, aes(RadPeer.Score, Technical.Performance.Score),
             shape = 10, size= 4, color = c('darkblue', 'darkred')) +
  labs(title = 'Scatter plot of Provider Score Means', size = '# Exams', color = 'Cluster')
```

The darker bulls-eyes in the graph above represents the centroids of the clusters.

### Compare clusters to other characteristics of the Providers

```{r}
SubSpecial <- data1 %>%
  select(Provider.ID, cluster1, cluster2) %>%
  inner_join(subspecial_data, by = 'Provider.ID')
SubSpecial %>% count(Is.Subspecialized, cluster1) %>% mutate(pct = percent(n/sum(n)), n = sum(n)) %>% spread(cluster1, pct) 
SubSpecial %>% count(Is.Subspecialized, cluster2) %>% mutate(pct = percent(n/sum(n)), n = sum(n)) %>% spread(cluster2, pct) 
```

Looking at subspecializations, there seems to be a notable difference, especially with the cluster 2 -- where less than 65% of the non-subspecialized providers are "good", but almost 80% of the sub-specialized ones are good


```{r}
equipData <- data1 %>%
  select(Provider.ID, cluster1, cluster2, nExams) %>%
  inner_join(equipment_data, by = 'Provider.ID')
equipData %>% 
  count(MRI.machine.type, cluster2) %>%
  mutate(pct = percent(n/sum(n)), n = sum(n)) %>%
  spread(cluster2, pct)
```

There seem to be more significant connections between `MRI.machine.type` and cluster, where 90.9% of Stand-Up machines get clustered as bad, while 83.1% of Semi-Open Wide Bore machines get clustered as good.  Open machines seem to be a bit more mixed, at 65% good, Closed machines are almost as good as Semi-Open Wide Bore machines, and there are not enough Extremity machines to get a good look

Finally, let's take a quick look at `MRI.magnet.strength`.

```{r}
equipData %>%
  ggplot(aes(MRI.magnet.strength)) +
  geom_density(adjust= 0.5, color = 'dodgerblue4', fill = 'dodgerblue4') +
  facet_grid(cluster2 ~ .) +
  theme_light() +
  coord_cartesian(xlim = c(0.2, 3)) +
  scale_x_continuous(expand = c(0, 0))
```

Looking at a smoothed density plot, the distribution of Magnet Strength between the two clusters are fairly similar, although the 'good' cluster seems to be a bit higher.

```{r}
logReg1 <- equipData %>%
  mutate(cluster2 = if_else(cluster2 == 'good', 1, 0)) %>%
  glm(data = ., 
      formula = cluster2 ~ MRI.magnet.strength, 
      family = binomial,
      weights = nExams)
plotData <- data.frame(MRI.magnet.strength = seq(0.2, 3, 0.01))
plotData <- predict(logReg1, newdata = plotData, type = 'response') %>%
  as.data.frame %>% bind_cols(plotData, .) %>%
  setNames(c('MRI.magnet.strength', 'fitted.y'))
equipData %>%
  mutate(cluster2 = if_else(cluster2 == 'good', 1, 0)) %>%
  ggplot(aes(x = MRI.magnet.strength, y = cluster2))+
  geom_jitter(height = 0.1, width = 0, alpha = 0.1, aes(size = nExams)) +
  geom_line(data = plotData, aes(x = MRI.magnet.strength, y = fitted.y, color = 'fitted glm')) +
  theme_light() +
  labs(title = 'Jitter plot with fitted logistic regression curve',
       size = '# Exams', y = 'Cluster 2 ("good" = 1)', color = '')

```

This logistic regression makes it a bit more obvious that a higher magnet strength leads to a higher likelihood of being classifies 'good'.
